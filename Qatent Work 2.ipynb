{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab63eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from get_SAO_en import *\n",
    "import json\n",
    "\n",
    "\n",
    "global preposition_list\n",
    "preposition_list = ['about', 'above', 'across', 'after', 'against', 'along', 'among', 'around', 'at', 'before', 'behind', 'between', 'beyond', 'but', 'by', 'despite', 'down', 'during', 'except', 'for', 'from', 'in', 'into', 'like', 'near', 'of', 'off', 'on', 'onto', 'out', 'over', 'past', 'plus', 'since', 'throughout', 'to', 'towards', 'under', 'until', 'up', 'upon', 'with', 'within', 'without']\n",
    "\n",
    "def extractTriplets(claims):\n",
    "    sents = sent_tokenize(claims, language=\"english\")\n",
    "    for sent in sents:\n",
    "        saos = get_SAO_en(sent)\n",
    "        if saos:\n",
    "            if saos != None:\n",
    "                return saos\n",
    "            else:\n",
    "                return []\n",
    "            \n",
    "def get_noun_chunks(claims, model=nlp):\n",
    "    res = []\n",
    "    doc = nlp(claims)\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # remove det and stop words as root\n",
    "        try:\n",
    "            while chunk[0].pos_==\"DET\" or chunk[0].text in stopwords or chunk[0].lemma_ in stopwords or \",\" in chunk[0].text:\n",
    "                chunk = chunk[1:]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        if len(chunk) == 1:\n",
    "            if chunk[0].lemma_ not in stopwords:\n",
    "                res.append(chunk)\n",
    "        else:\n",
    "            if not (chunk.root.lemma_ in stopwords):\n",
    "                res.append(chunk.root)\n",
    "                \n",
    "            if not (\"plurality of\" in chunk.text.lower() or any(t.replace('.','',1).isdigit() for t in chunk.text.split()) or (\"e.g.\" in chunk.text) or (len(chunk) > 8 and \",\" in chunk.text)): \n",
    "                res.append(chunk)\n",
    "\n",
    "            if len(chunk) > 3:\n",
    "                children_compound = [child.i for child in chunk.root.children if child.dep_ == \"compound\"]\n",
    "                if children_compound:\n",
    "                    idx = children_compound + [chunk.root.i]\n",
    "                    span = doc[min(idx): max(idx)+1]\n",
    "\n",
    "                    if span != chunk:\n",
    "                        res.append(span)\n",
    "\n",
    "                # to split some very long noun chunk which includes \",\" in it\n",
    "                if len(chunk) > 8 and \",\" in chunk.text:\n",
    "                        \n",
    "                    start_index = chunk[0].i\n",
    "                    curr_index = start_index + 1\n",
    "                    while curr_index < chunk[-1].i:\n",
    "                        if doc[curr_index].text == \",\":\n",
    "\n",
    "                            res.append(doc[start_index: curr_index])\n",
    "                            start_index = curr_index + 1\n",
    "                            curr_index = start_index + 1\n",
    "                        else:\n",
    "                            curr_index += 1\n",
    "                    res.append(doc[start_index:chunk[-1].i+1])\n",
    "\n",
    "    # convert all tokens into span type        \n",
    "    res = [(doc[term.i: term.i+1] if type(term)==type(doc[0]) else term) for term in res]\n",
    "\n",
    "    for i in range(len(res)-1):\n",
    "        curr_term = res[i]\n",
    "        next_term = res[i+1]\n",
    "        try:\n",
    "            if doc[curr_term[-1].i+1].text in preposition_list and next_term[0].i == (curr_term[-1].i+2):\n",
    "                res.append(doc[curr_term[0].i: next_term[-1].i+1])\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c573e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfPatents = open('texts_raw_2018.txt').read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7923aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfClaims = []\n",
    "for p in listOfPatents:\n",
    "    try:\n",
    "        if '_____c:' in p:\n",
    "            rest, claims = p.split('_____c:')\n",
    "            listOfClaims.append(claims)\n",
    "    except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50c6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129891 claims\n"
     ]
    }
   ],
   "source": [
    "print(len(listOfClaims),'claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(listOfClaims[999:1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb130258",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('verb_rel.json')\n",
    "data = json.load(f)\n",
    "listOfVerbsAnnotated = \"\"\n",
    "for i in data:\n",
    "    listOfVerbsAnnotated = listOfVerbsAnnotated + \" \" + i\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "256d265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import json\n",
    "import spacy,re\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "with open(\"US_stopwords.txt\") as inf:\n",
    "    stopwords_to_append = inf.read().splitlines()\n",
    "stopwords.update(stopwords_to_append)\n",
    "\n",
    "def cleaned_noun_chunk(noun):\n",
    "    noun_cleaned = re.sub(\"( comprising$)\", \"\", noun)\n",
    "    return noun_cleaned\n",
    "\n",
    "\n",
    "def get_passive_verbs(sentence, model=nlp):\n",
    "    sentence = sentence.replace(\"said\", \"the\")\n",
    "\n",
    "    doc = model(sentence)\n",
    "\n",
    "    ############################# PART2 #############################\n",
    "    # for passive form\n",
    "    listOfPassiveVerbs = []\n",
    "    passive_verbs = [w for w in doc if w.pos_ == \"VERB\" and [c for c in w.children if c.dep_ == \"nsubjpass\"]]\n",
    "    for verb in passive_verbs:\n",
    "        subject = [child for child in verb.children if child.dep_ == \"nsubjpass\"][-1]\n",
    "        listOfPassiveVerbs.append(verb.text)\n",
    "\n",
    "    return listOfPassiveVerbs\n",
    "\n",
    "\n",
    "def extractPassiveVerbs(claims):\n",
    "    sents = sent_tokenize(claims, language=\"english\")\n",
    "    for sent in sents:\n",
    "        passive = get_passive_verbs(sent)\n",
    "        if passive:\n",
    "            if passive != None:\n",
    "                return passive\n",
    "            else:\n",
    "                return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5903ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m listOfPassiveVerbs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m listOfClaims:\n\u001b[0;32m----> 5\u001b[0m     passive \u001b[38;5;241m=\u001b[39m \u001b[43mextractPassiveVerbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m passive \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         listOfPassiveVerbs\u001b[38;5;241m.\u001b[39mappend(passive[\u001b[38;5;241m0\u001b[39m])\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mextractPassiveVerbs\u001b[0;34m(claims)\u001b[0m\n\u001b[1;32m     35\u001b[0m sents \u001b[38;5;241m=\u001b[39m sent_tokenize(claims, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents:\n\u001b[0;32m---> 37\u001b[0m     passive \u001b[38;5;241m=\u001b[39m \u001b[43mget_passive_verbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m passive:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m passive \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mget_passive_verbs\u001b[0;34m(sentence, model)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_passive_verbs\u001b[39m(sentence, model\u001b[38;5;241m=\u001b[39mnlp):\n\u001b[1;32m     19\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m############################# PART2 #############################\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# for passive form\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     listOfPassiveVerbs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1020\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "listOfPassiveVerbs = []\n",
    "for c in listOfClaims:\n",
    "    passive = extractPassiveVerbs(c)\n",
    "    if passive is not None:\n",
    "        listOfPassiveVerbs.append(passive[0])\n",
    "\n",
    "Counter(listOfPassiveVerbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbsDict = {}\n",
    "for claims in listOfClaims:\n",
    "    try:\n",
    "        triplets = extractTriplets(claims)\n",
    "        if triplets is not None:\n",
    "            for triplet in triplets:\n",
    "                if triplet[1] not in listOfVerbsAnnotated:\n",
    "                    if triplet[1] in verbsDict:\n",
    "                        verbsDict[triplet[1]] = verbsDict[triplet[1]] + 1\n",
    "                    else:\n",
    "                        verbsDict[triplet[1]] = 1\n",
    "    except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in verbsDict.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fa3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc8758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72de94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

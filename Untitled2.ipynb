{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c680ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import json\n",
    "import spacy,re\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "with open(\"US_stopwords.txt\") as inf:\n",
    "    stopwords_to_append = inf.read().splitlines()\n",
    "stopwords.update(stopwords_to_append)\n",
    "\n",
    "def cleaned_noun_chunk(noun):\n",
    "    noun_cleaned = re.sub(\"( comprising$)\", \"\", noun)\n",
    "    return noun_cleaned\n",
    "\n",
    "\n",
    "def get_passive_verbs(sentence, model=nlp):\n",
    "    sentence = sentence.replace(\"said\", \"the\")\n",
    "\n",
    "    doc = model(sentence)\n",
    "    res = []\n",
    "\n",
    "    # get all noun chunks \n",
    "    nouns = [n for n in doc.noun_chunks]\n",
    "\n",
    "    ############################# PART1 #############################\n",
    "    # filter for dobj\n",
    "    nouns_dobj = [n for n in nouns if n.root.dep_ in [\"dobj\", \"appos\"]]\n",
    "\n",
    "    for n in nouns_dobj:\n",
    "        object = \" \".join([w.text for w in n if w.pos_!=\"DET\"])\n",
    "        verb = n.root.head\n",
    "        if verb.dep_ == \"conj\" and verb.head.pos_ == \"VERB\":\n",
    "            verb = verb.head\n",
    "\n",
    "        subject = None\n",
    "        # situation 1: acl \n",
    "        if verb.dep_ == \"acl\":\n",
    "            subject = verb.head\n",
    "            while subject.dep_ == \"acl\":\n",
    "                subject = subject.head\n",
    "\n",
    "        # situation 2: nsubj\n",
    "        else:\n",
    "            subject_node = [child for child in verb.children if child.dep_==\"nsubj\"]\n",
    "            if subject_node:\n",
    "                subject = subject_node[-1]  # suppose the sentence follows the order of subject-verb-object and \"-1\" refers to the subject which is closer to the dobj\n",
    "    \n",
    "            # situation 3: advcl + nsubj\n",
    "            elif verb.dep_ == \"advcl\":\n",
    "                subject_node = [child for child in verb.head.children if child.dep_==\"nsubj\"]\n",
    "                if subject_node: \n",
    "                    subject = subject_node[-1]\n",
    "\n",
    "        # find noun chunk that includes the subject noun\n",
    "        if subject:\n",
    "            if subject.pos_ == \"PRON\" and verb.dep_==\"relcl\":\n",
    "                subject = verb.head\n",
    "\n",
    "            try:\n",
    "                subject = [nn for nn in nouns if subject in nn][0]\n",
    "                subject = \" \".join([w.text for w in subject if w.pos_!=\"DET\"])\n",
    "            except IndexError:\n",
    "                subject = subject.text\n",
    "\n",
    "            objects = [object] \n",
    "\n",
    "            # check if object has other conjunct objects\n",
    "            conj_obj = [child for child in n.root.children if child.dep_==\"conj\"]\n",
    "            if conj_obj:\n",
    "                # go deeper (for conj that has another conj)\n",
    "                conj_obj_deeper = [child for c in conj_obj for child in c.children if child.dep_==\"conj\"]\n",
    "                while conj_obj_deeper:\n",
    "                    conj_obj.extend(conj_obj_deeper)\n",
    "                    conj_obj_deeper = [child for c in conj_obj_deeper for child in c.children if child.dep_==\"conj\"]\n",
    "        \n",
    "                for obj in conj_obj:\n",
    "                    try:\n",
    "                        obj_chunk = [nn for nn in nouns if obj in nn][0]\n",
    "                        if obj_chunk:\n",
    "                            objects.append(\" \".join([w.text for w in obj_chunk if w.pos_!=\"DET\"]))\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "\n",
    "            for object in objects:\n",
    "                if (subject not in stopwords and object not in stopwords) and (subject!=object):\n",
    "                    res.append((cleaned_noun_chunk(subject), verb.lemma_, cleaned_noun_chunk(object)))\n",
    "\n",
    "    ############################# PART2 #############################\n",
    "    # for passive form\n",
    "    listOfPassiveVerbs = []\n",
    "    passive_verbs = [w for w in doc if w.pos_ == \"VERB\" and [c for c in w.children if c.dep_ == \"nsubjpass\"]]\n",
    "    for verb in passive_verbs:\n",
    "        subject = [child for child in verb.children if child.dep_ == \"nsubjpass\"][-1]\n",
    "        listOfPassiveVerbs.append(verb.text)\n",
    "\n",
    "        try:\n",
    "            subject = [nn for nn in nouns if subject in nn][0]\n",
    "            subject = \" \".join([w.text for w in subject if w.pos_!=\"DET\"])\n",
    "        except (IndexError, AttributeError) as e:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            prep = [child for child in verb.children if child.dep_ == \"prep\"][-1]\n",
    "            pobjs = [child for child in prep.children if child.dep_ == \"pobj\"]\n",
    "\n",
    "            pobjs_conj = [child for pobj in pobjs for child in pobj.children if child.dep_==\"conj\"]\n",
    "            while pobjs_conj:\n",
    "                pobjs.extend(pobjs_conj)\n",
    "                pobjs_conj = [child for pobj in pobjs_conj for child in pobj.children if child.dep_==\"conj\"]\n",
    "\n",
    "            for object in pobjs:\n",
    "                try:\n",
    "                    object = [nn for nn in nouns if object in nn][0]\n",
    "                    object = \" \".join([w.text for w in object if w.pos_!=\"DET\"])\n",
    "                except IndexError:\n",
    "                   continue \n",
    "\n",
    "                if (subject not in stopwords and object not in stopwords) and (subject!=object):\n",
    "                    verb_text = \" \".join([w.text for w in doc if (w==verb) or (abs(w.i - verb.i)<=2 and  w.head==verb and w.dep_ in [\"auxpass\", \"prep\"])])\n",
    "                    verb_text = re.sub(\"(is|are|being)\",\"be\", verb_text)\n",
    "                    res.append((cleaned_noun_chunk(subject), verb_text, cleaned_noun_chunk(object)))\n",
    "            \n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    return listOfPassiveVerbs\n",
    "\n",
    "\n",
    "def extractPassiveVerbs(claims):\n",
    "    sents = sent_tokenize(claims, language=\"english\")\n",
    "    for sent in sents:\n",
    "        passive = get_passive_verbs(sent)\n",
    "        if passive:\n",
    "            if passive != None:\n",
    "                return passive\n",
    "            else:\n",
    "                return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1678fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfPatents = open('texts_raw_2018.txt').read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fe9ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfClaims = []\n",
    "for p in listOfPatents:\n",
    "    try:\n",
    "        if '_____c:' in p:\n",
    "            rest, claims = p.split('_____c:')\n",
    "            listOfClaims.append(claims)\n",
    "    except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0cb4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfPassiveVerbs = []\n",
    "for c in listOfClaims:\n",
    "    passive = extractPassiveVerbs(c)\n",
    "    if passive is not None:\n",
    "        listOfPassiveVerbs.append(passive[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69228b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'carried': 1, 'substituted': 1, 'attached': 1, 'reacted': 1})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(listOfPassiveVerbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c362b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
